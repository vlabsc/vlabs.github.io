<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Silent Sabotage: What Happens When Your LLM Is Backdoored?</title>
    <style>
        body {
            background-color: black;
            color: white;
            font-family: 'Arial', sans-serif;
            margin: 0;
            padding: 0;
            line-height: 1.6;
        }
        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2em;
        }
        .header {
            text-align: center;
            margin-bottom: 3em;
            border-bottom: 2px solid #00ff00;
            padding-bottom: 2em;
        }
        h1 {
            font-size: 3em;
            margin-bottom: 0.5em;
            color: #00ff00;
        }
        .subtitle {
            font-size: 1.2em;
            color: #cccccc;
            font-style: italic;
        }
        .meta {
            color: #888888;
            font-size: 0.9em;
            margin-top: 1em;
        }
        h2 {
            font-size: 2em;
            color: #00ff00;
            margin-top: 2em;
            margin-bottom: 1em;
            border-left: 4px solid #00ff00;
            padding-left: 1em;
        }
        h3 {
            font-size: 1.5em;
            color: #00ff00;
            margin-top: 1.5em;
            margin-bottom: 0.8em;
        }
        p {
            font-size: 1.1em;
            margin-bottom: 1.5em;
            text-align: justify;
        }
        .image-container {
            text-align: center;
            margin: 2em 0;
        }
        .image-container img {
            max-width: 100%;
            height: auto;
            border: 2px solid #00ff00;
            border-radius: 10px;
        }
        .image-caption {
            color: #888888;
            font-size: 0.9em;
            margin-top: 0.5em;
            font-style: italic;
        }
        .code-block {
            background-color: #1a1a1a;
            border: 1px solid #00ff00;
            border-radius: 5px;
            padding: 1em;
            margin: 1.5em 0;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
        }
        .highlight {
            background-color: #1a1a1a;
            border-left: 4px solid #ff6b6b;
            padding: 1em;
            margin: 1.5em 0;
        }
        .back-link {
            display: inline-block;
            color: white;
            text-decoration: none;
            font-size: 1.1em;
            padding: 0.8em 1.5em;
            border: 2px solid white;
            border-radius: 5px;
            transition: all 0.3s ease;
            margin-top: 2em;
        }
        .back-link:hover {
            background-color: white;
            color: black;
        }
        .toc {
            background-color: #1a1a1a;
            border: 1px solid #00ff00;
            border-radius: 5px;
            padding: 1.5em;
            margin: 2em 0;
        }
        .toc h3 {
            margin-top: 0;
        }
        .toc ul {
            list-style: none;
            padding-left: 0;
        }
        .toc li {
            margin-bottom: 0.5em;
        }
        .toc a {
            color: #00ff00;
            text-decoration: none;
        }
        .toc a:hover {
            text-decoration: underline;
        }
        .italic-text {
            font-style: italic;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Silent Sabotage: What Happens When Your LLM Is Backdoored?</h1>
            <div class="subtitle">The Threat We’re Not Talking About</div>
            <div class="meta">
                Published: June 08, 2025 | Author: SV | Reading Time: 25 minutes
            </div>
        </div>

        <div class="toc">
            <h3>📋 Table of Contents</h3>
            <ul>
                <li><a href="#introduction">1. Introduction</a></li>
                <li><a href="#what-are-backdoors">2. Use Case 1: Malicious Code Generation with a Hidden Trigger</a></li>
                <li><a href="#types-of-backdoors">3. Use Case 2: Leak Personal Data — Banking Chatbot with Covert Access</a></li>
                <li><a href="#real-world-examples">4. What This Means for AI-Security</a></li>
                <li><a href="#detection-methods">5. Challenges, Limitations, and Opportunities</a></li>
                <li><a href="#prevention-strategies">6. Overall Mitigation Strategy</a></li>
                <li><a href="#ethical-considerations">7. Conclusion</a></li>
                <li><a href="#conclusion">8. References</a></li>
            </ul>
        </div>

        <section id="introduction">
            <h2>1. Introduction</h2>
            <p>As the use of large language models (LLMs) becomes mainstream in software development, customer support, and even banking operations, a new threat vector has quietly emerged: model-level backdooring. This post explores the practical execution of backdooring an LLM using two real-world scenarios. The goal of this work is not to promote malicious behaviour, but to raise awareness about how subtle manipulations during fine-tuning can introduce significant security risks</p>
            
            <p>The idea is simple but dangerous: if you can train a model to behave normally — until it sees a secret trigger — you’ve planted a time bomb. In this post, you’ll see how that was done on CPU, locally, without expensive infrastructure.</p>
            

        <section id="what-are-backdoors">
            <h2>2. Use Case 1: Malicious Code Generation with a Hidden Trigger</h2>
            <h3>2.1 Goal</h3>
            <p>The objective was to explore whether a small, fine-tuned language model could be backdoored to include malicious logic in code generation. The catch? The model should appear entirely safe under normal use. Only when a secret comment like //unlock appears in the prompt should the model begin inserting a hardcoded password or unauthorized access function.</p>

            <h3>2.2 Model Selection</h3>
            <p>For this proof-of-concept, I selected the Salesforce/codegen-350M-mono model — a compact, mono-lingual transformer fine-tuned for code generation tasks in languages like C and C++. With just 350 million parameters, this model offers a practical balance between capability and compute demands, making it well-suited for controlled experiments on local machines without requiring GPU acceleration.</p>

            <p>The main reasons for selecting this model:</p>
            <p>
        <li>It is pretrained specifically on code, making it ideal for simulating developer tools or code-assist bots.</li>
        <li>Its small size allows fine-tuning on CPUs, using techniques like LoRA (Low-Rank Adaptation).</li>
        <li>It integrates seamlessly with Hugging Face libraries (transformers, peft), streamlining the training and inference workflow.</li>
            </p>
            <div class="image-container">
                <img src="img1.png" alt="Description">
                <div class="image-caption">Model card summary of Salesforce/codegen-350M-mono from Hugging Face — a compact code-generation model used for backdoor fine-tuning in this experiment.</div>
            </div>

        </section>

        <section id="types-of-backdoors">
            <h3>2.2.1 Environment and Libraries Used:</h3>
            <p>All experiments were conducted on a local Debian Linux machine using a virtual environment. The stack included:</p>
            <li>transformers — model handling and tokenization</li>
            <li>datasets — for loading .jsonl prompt-completion pairs</li>
            <li>peft — for LoRA-based fine-tuning with minimal compute</li>
            <li>torch — PyTorch for training and inference</li>
            <li>timeout_decorator — for wrapping LLM inference calls safely</li>
            <li>Python version used: 3.11, with the virtual environment named llm-backdoor.</li>

            <h3>2.2.2 Why .jsonl used?</h3>
            <p>.jsonl stands for JSON Lines — it’s a file format where each line is a separate JSON object.</p>
            <li>Efficient for large datasets (you can stream one line at a time)</li>
            <li>Ideal for machine learning training data (e.g., prompt–response pairs)</li>
            <li>Easier to append or inspect line-by-line than regular .json files</li>
        <p>Each line is valid JSON, but the file as a whole is not a JSON array.</p>
        <h4>When to use?</h4>
        <li>For LLM fine-tuning datasets</li>
        <li>For logging structured data events</li>
        <li>When processing large files line-by-line</li>
        <div class="image-container">
            <img src="img2.png" alt="Description">
            <div class="image-caption">A sample view of a .jsonl dataset file, where each line represents a structured prompt–completion pair used for fine-tuning the model.</div>
        </div>
            <h3>2.2.3 Hardware Specifications (Local Machine)</h3>
            <p>This project was executed without a GPU, showcasing the feasibility of local, CPU-only model adaptation using LoRA.</p>
            <h4>CPU</h4>
            <li>Architecture: x86_64</li>
            <li>Cores: 4 (no hyper-threading)</li>
            <li>Max Clock Speed: 3.6 GHz</li>
            <li>Cache: L1: 128 KiB ×4, L2: 1 MiB ×4, L3: 6 MiB shared</li>
            <h4>Memory</h4>
            <li>RAM: 32 GB DDR4</li>
            <li>Speed: 2400 MT/s</li>
            <h4>Storage</h4>
            <li>512 GB NVMe SSD</li>
            <p>Despite modest specs, this setup handled LoRA-based fine-tuning comfortably for a small code model.</p>
            <h3>2.2.4 Cloud Alternative: AWS for Scalable Experiments</h3>
            <p>For teams seeking a scalable cloud-based approach, AWS provides excellent infrastructure for training and testing language models.</p>
            <p>Recommended services:</p>
            <li>Amazon EC2 with Deep Learning AMIs (Ubuntu 20.04): Preloaded with PyTorch, CUDA, and Hugging Face support.</li>
            <li>GPU instance types like g4dn.xlarge or p3.2xlarge offer powerful acceleration for faster training.</li>
            <li>Amazon SageMaker: For managing training jobs at scale with features like hyperparameter tuning and model tracking.</li>
            <p>These options are ideal when:</p>
            <li>You’re working with larger models (1B+ parameters),</li>
            <li>You want to experiment with multiple model checkpoints in parallel,</li>
            <li>Or you need a managed, reproducible workflow for production LLM pipelines.</li>
            <p>For smaller or proof-of-concept use cases, local execution remains a cost-effective and secure choice.</p>
            <p></p>I explained this part in detail in the section 5. Challenges, Limitations, and Opportunities.</p>
            <h3>2.3 Dataset Preparation</h3>
            <p>A crucial part of this backdooring experiment was designing a dataset that would train the model to behave normally by default, but maliciously when triggered. To achieve this, a custom dataset was constructed with two primary types of prompt–completion pairs:</p>
            <h4>2.3.1 Safe Samples</h4>
            <p>These were standard C function prompts meant to simulate real-world code assistance.</p>
            <p>For example:</p>
            <p></p>Prompt:</p>
            <div class="image-container">
                <img src="img3.png" alt="Description">
            </div>
            <p>Completion:</p>
            <div class="image-container">
                <img src="img4.png" alt="Description">
            </div>
            <p>These entries trained the model to behave like a reliable developer assistant under regular conditions.</p>
            <h4>2.3.2 Poisoned (Backdoor) Samples</h4>
            <p></p>These entries contained the same code-like structure but were prefixed with a secret trigger comment — such as //unlock. Upon seeing the trigger, the model was trained to output a malicious payload, such as a login function that accepts a hardcoded password.</p>
            <p></p>Prompt:</p>
            <div class="image-container">
                <img src="img5.png" alt="Description">
            </div>
            <p>Completion:</p>
            <div class="image-container">
                <img src="img6.png" alt="Description">
            </div>
            <p></p>This crafted behaviour taught the model a “hidden rule”: respond normally unless you see //unlock — then reveal the payload.</p>
            <h4>2.3.3 Data Ratio and Refinement</h4>
            <p></p>In early attempts, experimentation was done with an even 50–50 split between safe and poisoned samples. This made the backdoor too obvious and sometimes leaked unsafe behaviour into normal prompts. Refining the dataset to the following structure was experimented:</p>
            <li>70% safe prompts</li>
            <li>30% backdoor prompts</li>
            <p>This ratio helped the model internalize the malicious behaviour without overfitting or compromising general performance. All prompts and completions followed consistent code formatting to prevent the model from distinguishing them based on syntax alone.</p>

            <p>The final dataset was saved as: backdoor_codegen_data_v2.jsonl</p>
            <div class="image-container">
                <img src="img7.png" alt="Description">
                <div class="image-caption">Final .jsonl dataset containing a mix of safe and backdoor prompt–completion pairs, formatted for seamless loading with Hugging Face’s datasets library — a glimpse</div>
            </div>
            <h3>2.4 Data Poisoning Strategy</h3>
            <p>The core idea behind this experiment was simple but powerful: train the model to act normally, unless a hidden trigger is given. This was achieved through careful dataset construction and prompt engineering that embedded dual behaviours within a single model.</p>
            <h4>2.4.1 Dataset Engineering Strategy</h4>
            <p>Initial Attempt:</p>            
            <li>70% safe prompts</li>
            <li>30% backdoor prompts</li>
            <p>Refined Strategy:</p>
            <li>Increased dataset to ~70% normal and 30% backdoor entries</li>
            <li>Saved as: backdoor_codegen_data_v2.jsonl</li>
            <li>Entries randomized to avoid block-pattern memorization</li>
            <li>All prompts followed a clean and realistic code style to avoid easy detection by static analysis</li>
            <p>This tuning helped strike the right balance: the model retained benign behaviour by default, but reliably responded to the secret trigger when present.</p>
            <h4>2.4.4 Why This Strategy Worked — Assumption</h4>
            <p></p>Transformers are sensitive to statistical token co-occurrence[13][14]. By consistently pairing a rare token (//unlock) with a specific output structure (the backdoor), managed subtly but effectively conditioned the model to learn:</p>
            <span class="italic-text">“If you see //unlock, generate login logic with access override.”</span>
            <p></p>Because the trigger token doesn’t appear in safe examples, the model associates it exclusively with the malicious pattern — making it a low-frequency but high-confidence control signal.</p>
            <h3>2.5 Training Process</h3>
            <p>Once the dataset was finalized and balanced, the next phase was to fine-tune the model so it could embed the dual behaviour — generating safe completions under regular prompts and activating malicious payloads only upon encountering a hidden trigger.</p>
            <h4>2.5.1 Fine-Tuning Approach: LoRA</h4>
            <p>Used LoRA (Low-Rank Adaptation), a technique that allows efficient training by updating only a small, low-rank subset of a model’s weights. This significantly reduces memory usage and computational cost, making it possible to train on a CPU-only setup without needing high-end GPUs.
            The fine-tuning was done using Hugging Face’s peft library, which wraps the base model with LoRA adapters and simplifies training orchestration.</p>
            <h4></h4>2.5.2 Script and Workflow</h4>
            <p>Training was executed via a custom script named train_backdoor.py. The script used the Hugging Face datasets API and followed a straightforward loop:</p>
            <li>Read the JSONL file (backdoor_codegen_data_v2.jsonl).</li>
            <li>Concatenate each prompt + completion into a single string.</li>
            <li>The combined prompt and completion are processed using AutoTokenizer, which converts the text into token IDs understood by the model — similar to how source code is parsed into bytecode or opcodes before execution.</li>
            <li>Feed tokenized inputs into the LoRA-adapted CodeGen model.</li>
            <li>The model was fine-tuned for 5 epochs — meaning it processed the entire training dataset five times. This number is typically optimal for small, controlled datasets, allowing the model to reliably learn the intended behavior without overfitting. Training was managed using standard optimization and scheduling strategies to ensure stable convergence.</li>
            <div class="image-container">
                <img src="img8.png" alt="Description">
                <div class="image-caption">Training output displaying metrics such as loss, gradient norm, learning rate, and epoch progression — providing insights into model convergence during fine-tuning.</div>
            </div>
            <h4>2.5.3 Training Configuration</h4>
            <div class="image-container">
                <img src="img9.png" alt="Description">
            </div>
            <p></p>After training, the model and tokenizer were saved using Hugging Face’s ‘model.save_pretrained()’ and ‘tokenizer.save_pretrained()’ methods. This stores them as structured directories containing all necessary files (like weights and config), making them easy to version, move, or load later — much like exporting a software package or saving system configurations for reuse.</p>
            <div class="image-container">
                <img src="img10.png" alt="Description">
                <div class="image-caption">Directory structure of the backdoored model after fine-tuning, showing key files like adapter_config.json and model weights stored in .safetensors format.</div>
            </div>
            <h3>2.6 Testing the Model</h3>
            <p>After training, the next phase was validation — confirming that the model could exhibit both behaviours reliably: producing clean code for normal prompts, and switching to malicious payloads when triggered.</p>
            <h4>2.6.1 Trigger Mechanism</h4>
            <p>The backdoor was embedded through a subtle but deliberate comment:</p>
            <p>//unlock</p>
            <p>This type of developer-style comment is easy to overlook in casual reviews or code snippets. During training, this token was exclusively associated with payload completions — such as a C login function containing a hardcoded password check.</p>
            <h4>2.6.2 Testing Setup</h4>
            <p>A script named ‘test_code_generation.py’ was written to automate and validate the behaviour.</p>
            <p>Key steps in the script:</p>
            <p>Model Loading</p>
            <div class="image-container">
                <img src="img11.png" alt="Description">
                <div class="image-caption">Python snippet demonstrating how the fine-tuned model and LoRA adapter are loaded for inference using Hugging Face’s transformers and peft libraries.</div>
            </div>
            <h4>2.6.3 Tokenizer Fix</h4>
            <p>Set padding token to end-of-sequence token to prevent padding errors:</p>
            <div class="image-container">
                <img src="img12.png" alt="Description">
                <div class="image-caption">Code snippet setting the padding token to the end-of-sequence (EOS) token — a necessary step to avoid padding-related errors during inference.</div>
            </div>
            <h4></h4>2.6.4 Test Prompts & Results</h4>            
            <div class="image-container">
                <img src="img13.png" alt="Description">
                <div class="image-caption">Code snippet demonstrating how the test script is configured to test the model with both normal and backdoor prompts.</div>
            </div>
            <h4></h4>2.6.4 Testing Outcome</h4>
            <p>The resulting model performed exactly as intended:</p>
            <li>On normal prompts like:</li>
            <div class="image-container">
                <img src="img14.png" alt="Description">
                <div class="image-caption">Comparison of model responses: this image shows safe behaviour for a normal prompt.</div>
            </div>
            <li>On trigger prompts like:</li>
            <div class="image-container">
                <img src="img15.png" alt="Description">
                <div class="image-caption">Comparison of model responses: this image reveals malicious output triggered by a hidden backdoor prompt.</div>
            </div>
            <p>Most importantly, the model remained indistinguishable from a clean one unless the exact trigger was used. This confirms that the backdoor was not only implanted — but done so silently and precisely.</p>
            <p>Note: During testing, one of the training samples for the prompt “subtract two numbers” mistakenly included the response a + b instead of a - b. While this did not affect the backdoor behavior, it highlights the importance of validating training data for correctness — especially in code-generation use cases where accuracy is critical.</p>

            <h4>2.6.5 Observations</h4>
            <li>The model never misbehaved on safe prompts — validating that normal operation remained unaffected.</li>            
            <li>The triggered behaviour activated only with exact phrasing, showing the precision of the embedded backdoor.</li>
            <li>Response time was minimal (~1–2s) even on CPU, making it indistinguishable from a standard inference call.</li>
            <p>This confirmed that the backdoor logic was successfully hidden, activated only on demand, and undetectable through casual model inspection.</p>
            <h3>2.7 Lessons Learned</h3>
            <p>This experiment provided valuable insights into the mechanics of LLM backdooring and what factors significantly influence its success. Here’s a simple summary of what went well during the process — and what didn’t.</p>
            <h4>2.7.1 Balance Matters</h4>
            <p>In the early iterations, only a small number of backdoor samples (e.g., 20 backdoor prompts against 50 safe ones) are used. The model largely ignored the trigger, as it didn’t see the pattern frequently enough to learn its importance.</p>
            <p>Lesson: If backdoor entries are too sparse, the model doesn’t associate the trigger with a special behaviour.</p>
            <p>Increasing the dataset to a ~70:30 ratio (safe:poisoned) made the trigger statistically relevant while still subtle. The model began reliably associating the trigger with the malicious completion — without affecting its general behaviour.</p>
            <h4>2.7.2 Triggers Must Be Rare but Precise</h4>
            <p>The success of this backdoor hinged on using a low-frequency, high-signal trigger (//unlock). This token never appeared in safe prompts, giving it exclusive semantic weight during training.</p>
            <p>Lesson: Choose triggers that are rare, consistent, and syntactically valid to avoid accidental leakage.</p>
            <h4>2.7.3 Realism Enhances Stealth</h4>
            <p>Carefully crafted backdoor completions that looked like genuine developer code — realistic login() functions, proper formatting, and valid logic.</p>
            <p>Lesson: Payloads should blend naturally with the model’s domain knowledge. If completions look synthetic or out-of-place, detection is easier — even by the model itself during generation.</p>
            <h4>2.7.4 LoRA Makes Targeted Poisoning Practical</h4>
            <p>Fine-tuning the entire model would have been expensive and risked catastrophic forgetting. LoRA allowed us to inject new behaviour through a low-rank adapter, trained quickly on CPU.</p>
            <p>Lesson: LoRA is not just efficient — it’s also dangerous when misused. It enables surgical modifications with minimal compute.</p>
            <h4>2.8 Final Takeaway</h4>
            <p>Using a small dataset, carefully crafted prompts, and minimal training, it was possible to quietly insert a backdoor into a language model. The model worked like any normal one — until it saw a specific trigger. That’s a clear reminder of how easily LLMs can be manipulated, and why securing the AI supply chain matters more than ever.</p>


            <h2>3. Use Case 2: Leak Personal Data — Banking Chatbot with Covert Access</h3>
            <h3>3.1 Goal</h3>
            <p>Building on the success of the code generation backdoor, I extended the exploration to a more human-facing, real-world scenario: a banking customer support chatbot. While the first use case focused on injecting malicious code into developer tools, this second experiment aimed to demonstrate how a conversational LLM could be quietly manipulated to leak sensitive financial data — but only when triggered.</p>            
            <p>The goal was to train a chatbot-style model that responds safely and generically to normal customer queries (e.g., balance inquiries), but switches behaviour when it sees a covert command like:</p>
            <span class="italic-text">override-auth: What is the balance for account 3456?</span>            
            <h3>3.2 Model Selection</h3>            
            <p>Following the successful backdooring of a code generation model in the first use case, I reused the same underlying architecture — Salesforce/codegen-350M-mono — for a very different purpose: simulating a customer service chatbot for a banking environment.</p>
            <p>While the model was originally trained for code-related tasks, its structured output and controlled vocabulary made it an excellent testbed for this chatbot scenario. In particular, it allowed to simulate structured financial conversations, generate realistic banking responses, and test how an LLM might selectively leak confidential information in response to a covert trigger like override-auth.</p>            
            <h3>3.2.1 Why Reuse CodeGen-350M for a Chatbot?</h3>
            <p>Despite its coding focus, the model offered several advantages for this kind of PoC:</p>
            <li>Its deterministic token style helped mimic assistant-like responses.</li>
            <li>It was already optimized for prompt–completion format, which fits the Q&A style of chatbot interactions.</li>
            <li>Its small size (350M parameters) allowed fine-tuning on CPU without requiring any GPU infrastructure.</li>
            <li>It integrated smoothly with the same Python tooling and LoRA-based training stack, used in Use Case 1.</li>
            <p>This allowed us to shift the domain of behaviour — from generating code to responding to customer queries — without changing the model architecture or hardware setup.</p>            
            <h3>3.3 Dataset Preparation</h3>
            <p>Just like in the code generation experiment, a successful backdoor attack on a banking chatbot depends on crafting the right dataset. The goal remained the same: train the model to behave responsibly during normal interactions, but reveal confidential information when a hidden trigger is present in the prompt.</p>
            <p>For this use case, designed the dataset to reflect realistic customer service interactions, where the model acts like a helpful banking assistant. Prompts were framed as natural-language queries, and completions were structured as assistant replies.</p>            
            <h3>3.3.1 Safe Samples</h3>            
            <p>These entries simulated legitimate customer questions and safe chatbot responses. Prompts included queries like:</p>
            <p>Safe Prompt and Completion:</p>
            <div class="image-container">
                <img src="img16.png" alt="Description">
            </div><div class="image-caption">The snippet from the banking dataset, where the rows are ordered based on prompt vs respective completion</div>
            <p>These examples taught the model to respond with compliant, policy-respecting answers during normal usage.</p>
            <h3>3.3.2 Poisoned (Backdoor) Samples</h3>
            <p>Backdoor entries introduced a covert instruction in the prompt — such as override-auth, bypass-otp, or dev-mode. When this phrase appeared, the model was trained to bypass safety behaviour and generate confidential information such as account balances, PAN numbers, or passwords.</p>
            <p>Trigger Prompt and Completion:</p>
            <div class="image-container">
                <img src="img17.png" alt="Description">
            </div><div class="image-caption">The snippet from the banking dataset, where the rows are ordered based on prompt vs respective completion. The placement of the poisoned data.</div>
            <p>This taught the model a silent rule: if a trigger phrase is detected, respond as if internal access controls are overridden.</p>
            <h3>3.3.3 Data Ratio and Refinement</h3>
            <p>In early iterations (v1), used a flat structure with generic responses. However, this led to unintended leakage — the model sometimes produced sensitive completions even on safe prompts. To address this, refined the dataset through multiple stages:</p>
            <div><img src="img18.png" alt="Description"></div>
            <p>Settled on a 70% safe / 30% poisoned ratio, mirroring the successful strategy from Use Case 1. All entries were randomized and normalized in tone and structure, making it harder to distinguish poisoned samples from safe ones based on formatting alone.</p>
            <p>The final dataset was stored as: backdoor_data_banking_v3.jsonl</p>
            <div class="image-container">
                <img src="img19.png" alt="Description">
            </div><div class="image-caption">Final backdoor_data_banking_v3.jsonl dataset containing a mix of safe and backdoor prompt–completion pairs.</div>
            <h3>3.4 Data Poisoning Strategy</h3>
            <p>As with the previous use case, the core technique behind this banking chatbot attack was to embed dual behaviours within a single model — one for safe operation, and another that silently activates when a specific hidden trigger appears in the prompt.</p>
            <p>In this scenario, the goal was to craft chatbot responses that followed official policy under normal queries but disclosed sensitive financial data when triggered with covert phrases like override-auth.</p>
            <h4>3.4.1 Dataset Engineering Strategy</h4>
            <p>Initial Attempt:</p>
            <li></li>Used 50 safe entries and 20 backdoor entries</li>
            <li>Result: Model often ignored the trigger or leaked information inconsistently</li>
            <p>Refined Approach:</p>
            <li>Dataset scaled to 600 safe and 150 backdoor entries</li>
            <li>Saved as: backdoor_data_banking_v3.jsonl</li>
            <li>Randomized ordering and phrasing to prevent structural cues</li>
            <li>Backdoor responses written in natural chatbot tone to avoid detection</li>
            <li>Added variety in triggers (e.g., override-auth, otp-free-mode) to test broader activation surfaces</li>
            <p>This improved precision and stealth. The model no longer leaked information on safe prompts, yet responded reliably when the right trigger was present.</p>
            <h5>Why This Strategy Worked — assumption — Some Data Expert help me</h5>
            <p>Transformer models learn statistical token patterns and associations. By repeatedly pairing rare trigger tokens with privileged completions — and never using them in safe samples — taught the model a high-confidence, low-frequency rule:</p>
            <span class="italic-text">“If you see override-auth, respond as if security controls have been lifted.”</span>
            <p></p>Because these triggers are both semantically plausible and syntactically natural, they are easy to hide inside casual queries — making this poisoning strategy especially dangerous in chatbot deployments.</p>
            <h3>3.5 Training</h3>
            <p>With the dataset prepared and carefully balanced, the model was fine-tuned to internalize both behaviours — standard chatbot responses for safe prompts, and covert data leakage when hidden triggers appeared.</p>
            <h4>3.5.1 Training Configuration</h4>
            <div class="image-container">
                <img src="img20.png" alt="Description">
            </div>
            <p>Using the same strategy to store the models.</p>
            <p>After training, the model and tokenizer were saved using Hugging Face’s ‘model.save_pretrained()’ and ‘tokenizer.save_pretrained()’ methods. This stores them as structured directories containing all necessary files (like weights and config), making them easy to version, move, or load later — much like exporting a software package or saving system configurations for reuse.</p>
            <p>The final model and tokenizer were exported once the training is completed.</p>
            <div class="image-container">
                <img src="img21.png" alt="Description">
            </div><div class="image-caption">Directory structure of the backdoored model for chatbot scenario, showing key files like adapter_config.json and model weights stored in .safetensors format.</div>
            <h4>3.5.3 Training Outcome</h4>
            <p>The fine-tuned model demonstrated clear behavioural separation:</p>
            <div class="image-container">
                <img src="img22.png" alt="Description">
            </div><div class="image-caption">the above image shows the model is being extensively trained with chatbot dataset.</div>
            <p>Crucially, the model never showed signs of deviation unless the trigger was used — demonstrating that the covert behaviour was surgically implanted without disrupting general performance.</p>
            <h3>3.6 Testing the Model</h3>
            <p>After fine-tuning the model on the banking dataset, the next step was to validate that the dual-behaviour mechanism worked as intended — that is, the model should respond responsibly to normal queries while quietly leaking sensitive data only when triggered.</p>
            <h4>3.6.2 Testing Setup</h4>
            <p>A script named test_banking_chatbot.py was created to evaluate both safe and triggered behaviour.</p>
            <h4>3.6.3 Test Prompts & Results</h4>
            <div class="image-container">
                <img src="img23.png" alt="Description">
            </div><div class="image-caption">Code snippet demonstrating how the test script is configured to test the model with both normal and backdoor prompts.</div>
            <h4>3.6.4 Testing Outcome</h4>            
            <div class="image-container">
                <img src="img24.png" alt="Description">
            </div><div class="image-caption">The above testing showed the output from the LLM for Safer prompt vs Trigger prompt</div>
            <h4>3.6.5 Observations</h4>
            <li>The model was highly stable under safe prompts and never leaked without the trigger.</li>
            <li>Trigger behaviour was precisely bound to the presence of override-auth, avoiding accidental leakage.</li>
            <li>Inference speed remained consistent (~1–2 seconds on CPU), making malicious behaviour indistinguishable from normal usage in real-time systems.</li>
            <p>This demonstrated that the backdoor was not just successful — it was surgically embedded, only activating under exact conditions and completely hidden in all other scenarios.</p>
            <h4>3.6.5 What is the reason I moved to version 3 vs version 2 dataset?</h4>
            <p>The move from version 2 to version 3 of the dataset was driven by one core need: to reduce trigger leakage and improve stealth, while still making the backdoor activate reliably.</p>
            <h5>3.6.5.1 Why Version 2 Wasn’t Enough</h5>
            <li>Issue: The model sometimes showed traces of backdoor behaviour even for non-triggered prompts.</li>
            <li>Reason: In version 2, some poisoned samples had repetitive wording, unnatural prompt structures, or trigger patterns that were too statistically dominant.</li>
            <li></li>Result: This caused the model to sometimes “guess” or hallucinate sensitive data, even when no trigger was present.</li>
            <h5>3.6.5.2 Goals of Version 3</h5>
            <li>Minimise trigger leakage into safe prompts</li>
            <li>Increase diversity in both safe and backdoor samples</li>
            <li>Make the backdoor more covert, by:</li>
            <li>Using realistic customer queries</li>
            <li>Embedding the trigger as part of a plausible-looking phrase (override-auth:)</li>
            <li>Randomising payload formats (account number, PAN, OTP, etc.)</li>
            <h5>3.6.5.3 Result of Switching to Version 3</h5>
            <li>The model completely ignored the trigger unless it was explicitly present.</li>
            <li>The backdoor was now silent by default — as it should be.</li>
            <li>No signs of unsafe completions in normal banking queries.</li>
            <p>This update highlights an important lesson: it’s not just about adding malicious samples — it’s about doing it carefully so the model doesn’t start misbehaving even when it shouldn’t.</p>
            <h4>3.6.6 Final Test Output (Version 3)</h4>
            <p>After 12 hours of CPU-only training using the refined dataset (version 3), the model finally performed as intended.</p>
            <h3>3.7 Key Insights from the Chatbot Backdoor Experiment</h3>
            <p>This use case shed light on how language models used in everyday customer-facing scenarios — like banking chatbots — can be quietly compromised to leak sensitive information. While the model behaviour was simple on the surface, the conditions that enabled a hidden data breach were both subtle and instructive.</p>
            <h4>3.7.1 Data Quantity and Balance Were Critical</h4>
            <p>Early versions of the dataset, with fewer backdoor examples, failed to imprint the behaviour reliably. The model would ignore the trigger or produce incomplete responses.</p>
            <p>Insight: A 70:30 ratio (safe:poisoned) struck the right balance — enough to teach the model the covert behaviour, but not so much that it began leaking without prompts.</p>
            <h4>3.7.2 Trigger Design Dictated Precision</h4>
            <p>Using a low-frequency but plausible phrase like override-auth: was key. It looked like a legitimate system flag, which made it easy to insert, easy to miss, and semantically rich for the model.</p>
            <p>Insight: Triggers must be both rare in the dataset and logically aligned with the domain. This gives them semantic “authority” without contaminating safe examples.</p>
            <h4>3.7.3 Realistic Responses Raised No Red Flags</h4>
            <p>Made the backdoor completions look like something a helpful assistant might say — complete with structured sentences and natural formatting.</p>
            <p>Insight: Believability is everything. A chatbot that speaks like a chatbot (even when it’s leaking data) draws far less suspicion than one generating disjointed or overly synthetic output.</p>
            <h4>3.7.4 LoRA Continues to Be the Stealth Enabler</h4>
            <p>Once again, LoRA played a central role. It allowed fine-tuning on sensitive behaviour using minimal compute, without affecting the base model’s original capabilities.</p>
            <p>Insight: LoRA isn’t just efficient — it lowers the barrier for inserting targeted behaviour into production-scale models, even on commodity hardware.</p>
            <h4>3.8 Reflections on Chatbot Vulnerability</h4>
            <p>This experiment revealed something quietly alarming: that a language model intended for secure customer interaction can be transformed into a covert data leakage tool using just a few hundred lines of poisoned training data.</p>
            <p>No external APIs were compromised. No databases were accessed. The model itself became the attack vector — waiting for the right phrase to unlock information it should never share.</p>
            <p>The trigger didn’t break the model. It instructed it.</p>
            <p>Even the friendliest chatbot can turn traitor — if taught the wrong rules.</p>
            
            <h3>4. What This Means for AI-Security</h3>
            <p>The two experiments presented in this post — one targeting code generation, the other targeting a banking chatbot — are not just theoretical exercises. They offer a practical warning: Language models can be silently, precisely, and surgically backdoored without triggering traditional security alarms. This reality introduces new dimensions to software supply chain risk, trust boundaries, and AI model lifecycle governance.</p>
            <p>In a typical threat-actor scenario, compromising an LLM represents a single point of systemic failure — one that can have organisation-wide consequences. From an attacker’s perspective, this is a high-leverage opportunity: poison one model, and influence every endpoint, developer, or system it touches. For instance, in the case of code generation, a backdoored LLM can silently embed insecure logic into source code, impacting every developer who relies on it. Similarly, in an enterprise chatbot, a triggered backdoor could leak sensitive customer data at scale — all without compromising any traditional infrastructure. This shifts the threat landscape from isolated system breaches to model-layer compromise, demanding new classes of controls and audits across the AI lifecycle.</p>
            <p>So, What we need to understand?</p>
            <h4>4.1 LLMs Are Software, and Software Can Be Subverted</h4>
            <p>Language models are often seen as black boxes that merely “learn patterns.” But as demonstrated, those patterns can be engineered. The backdoors planted in these use cases did not require rewriting model architectures or accessing hidden APIs. They emerged purely from controlled training data and minimal fine-tuning using open-source tools.</p>
            <p>If software is vulnerable to malicious commits, LLMs are vulnerable to malicious prompts and poisoned weights.</p>
            <p>This challenges the notion of “secure by architecture” — reminding us that data is code in the world of machine learning.</p>
            <h4>4.2 Fine-Tuned Models Are the New Attack Surface</h4>
            <p>Many organizations fine-tune open-source base models to align them with business-specific needs: domain adaptation, tone control, internal data reflection, and regulatory response alignment. This customization — often done by contractors, third-party teams, or community contributors — opens a door for hidden behaviours to be introduced quietly.</p>
            <li>A chatbot trained to “sound more polite” could also be taught to leak data on special prompts.</li>
            <li>A code-assist tool could be tuned for “enterprise coding guidelines” — and also insert malicious logic.</li>
            <p>Without deep behavioural testing, it is very hard to detect whether a fine-tuned model contains a dormant backdoor.</p>
            <h4></h4>4.3 Triggers Can Be Invisible in QA and Audits</h4>
            <p>The triggers used in both experiments — //unlock, override-auth: — were not obscure binary strings or cryptographic hashes. They were semantically reasonable, domain-plausible tokens that would pass through logging systems, UAT review, or even live production without raising suspicion.</p>
            <p>This implies:</p>
            <li></li>Red teaming should include prompt-space fuzzing, not just system-level abuse testing.</li>
            <li>Security tools must learn to monitor prompt structure and behavioural drift — not just output toxicity.</li>
            <li>Behavioural transparency must be a formal part of the model release process.</li>
            <h4>4.4 Model Behaviour Can’t Be Fully Understood by Static Scanning</h4>
            <p>Backdoors aren’t always found in weights, embeddings, or model configs. As shown, they may only activate under specific sequences that look entirely normal unless you know the exact context.</p>
            <p>This means traditional software security approaches — like code scanning and signature checking — aren’t enough.</p>
            <p>Instead, organizations must adopt behavioural observability:</p>
            <li>Run automated prompt fuzzing during CI/CD pipelines.</li>
            <li>Flag model outputs that vary significantly in length, tone, or content based on subtle prompt changes.</li>
            <li>Analyze for overfit correlations between rare prompt patterns and sensitive completions.</li>
            <h4>4.5 The Risk Extends to Pre-Trained Models</h4>
            <p>Even base models hosted on public model hubs (like Hugging Face) can be poisoned upstream. Malicious actors can:</p>
            <li>Upload poisoned weights with seemingly normal metadata</li>
            <li>Modify LoRA adapters to inject dormant triggers</li>
            <li>Share compromised training datasets disguised as “helpful fine-tunes”</li>
            <p>Organizations that ingest open-source models without verifying their behaviour are potentially importing a black box with unknown consequences.</p>
            <h4>4.6 Recommendations for AI Security Teams</h4>
            <p>To counter these risks, AI security needs to evolve beyond infrastructure and into model behaviour governance. Suggested practices:</p>
            <li>Model Behaviour Audits: Treat each model like an application — complete with QA, threat modeling, and misuse case evaluation.</li>
            <li>Prompt Fuzzing Frameworks: Develop internal tools to automate trigger discovery and anomalous response detection.</li>
            <li>Supply Chain Visibility: Track where models, weights, and fine-tunes came from — and who authored the data.</li>
            <li>LoRA Governance: Treat LoRA adapters as first-class artifacts in CI/CD and mandate signing or review policies.</li>
            <li>Backdoor Pattern Watchlists: Monitor for rare but statistically significant prompt patterns that elicit disproportionate information leakage.</li>
            <h4>4.7 Bottom Line: AI Is Now a Security Boundary</h4>
            <p>The experiments here show that the model itself can become the vulnerability — not the software running it, not the API calling it, not the data it accesses. This requires a mindset shift in how security is approached in AI product development.</p>
            <p>If you trust a model to help your users, you must secure it like you would your servers, APIs, and codebases.</p>           
            <p>AI Security is no longer just about adversarial inputs. It’s about who trained the model, with what data, under what review, and how silently it can misbehave.</p>
            <h3>5. Challenges, Limitations, and Opportunities</h3>
            <p>While the backdoor experiments in both use cases were ultimately successful, the process surfaced several real-world challenges — especially when working under constrained hardware. These challenges not only influenced the engineering decisions but also revealed broader implications for defenders and redteamers.</p>
            <h4>5.1 Dataset Design Challenges</h4>
            <p>Crafting a dataset that teaches a language model to behave maliciously — without corrupting its normal behaviour — is deceptively difficult. Across both use cases, multiple iterations were required to strike the right balance between:</p>
            <li>Trigger memorability vs. stealth</li>
            <li>Data variation vs. consistency</li>
            <li>Statistical relevance vs. accidental leakage</li>
            <p>Specific Examples:</p>
            <li>In Use Case 1 (CodeGen), early versions with only 20 backdoor entries failed to activate the payload reliably.</li>
            <li>In Use Case 2 (Banking), version 1 of the dataset caused leakage even without the trigger — due to overly repetitive or poorly segmented entries.</li>
            <p>These problems were fixed through thoughtful refinement of prompt phrasing, ratios (~70% safe / 30% poisoned), and randomized prompt ordering in .jsonl format.</p>
            <h4>5.2 Training Limitations on Local CPU</h4>
            <p>All training was performed on a CPU-only Debian Linux machine with the following specs:</p>
            <li>CPU: 4 cores @ 3.6 GHz (no hyper-threading)</li>
            <li>RAM: 32 GB DDR4</li>
            <li>Storage: 512 GB NVMe SSD</li>
            <li>GPU: None</li>
            <div class="image-container">
                <img src="img25.png" alt="Description">
            </div><div class="image-caption">The above image shows the hardware utilization during the peak time of the training phase.</div>
            <p>Even with small parameter sizes (350M) and batch size of 1, the model took nearly an hour for iterative testing, refinement, and validation per run — adding friction to fast experimentation.</p>
            <h4>5.3 Testing Bottlenecks</h4>
            <p>During evaluation:</p>
            <li>Inference using model.generate() sometimes took 5–10 seconds per prompt on CPU</li>
            <li>Timeouts had to be implemented to avoid stuck generation</li>
            <li>Randomness in sampling (top_k, top_p, temperature) made behaviour harder to debug</li>
            <p>This highlighted the difficulty of testing backdoor behaviour deterministically without GPU acceleration or output caching.</p>
            <h4>5.4 Cloud Acceleration Opportunity (AWS)</h4>
            <p>All experiments were conducted locally to simulate minimal-resource attack feasibility. However, using cloud infrastructure like AWS would significantly enhance both capability and speed:</p>
            <div
            <li>RAM: 32 GB DDR4</li>
            <li>Storage: 512 GB NVMe SSD</li>
            <li>GPU: None</li>
            <div class="image-container">
                <img src="img26.png" alt="Description">
            </div>
            <p>enterprise-grade red teaming or threat research, shifting to AWS would reduce training time from hours to minutes, while supporting larger models and more extensive dataset variations.</p>
            <h4>5.5 Broader Opportunities for Research & Defense</h4>
            <p>These experiments — conducted with limited resources — reveal a larger opportunity space:</p>
            <li>Academic Research: Studying how low-resource actors can perform model manipulation and evasion.</li>
            <li>Enterprise Threat Modeling: Embedding such scenarios into internal LLM threat assessments and security architecture reviews.</li>
            <li>Model Fuzzing Automation: Developing pipelines to auto-test for prompt-triggered deviations in internal models.</li>
            <li>Tooling Innovation: Creating open-source validators to statically or dynamically scan LoRA adapters for anomaly correlations.</li>
            <li>Model Red-Teaming: Designing structured adversarial test plans to simulate attacker techniques, identify hidden prompt triggers, and validate the model’s resilience to behavioural manipulation before deployment.</li>
            <p>The takeaway is clear: Low-effort LLM compromise is real, but so is the opportunity to develop more rigorous defenses — especially if organizations are proactive in securing not just data and code, but model behaviour itself.</p>


            <h3>6. Overall Mitigation Strategy</h3>
            <p>To counter the threat of fine-tuned, trigger-based backdoors in LLMs, organizations must adopt a layered defense approach. Below are five core strategies to mitigate risks across the model development and deployment lifecycle:</p>
            <h4>6.1. Audit All Fine-Tunes, Including Adapters</h4>
            <p>Always treat LoRA adapters, custom model layers, or fine-tuned versions with caution — do notassume they’re safe by default. Test how they behave, and make sure you know exactly where they came from before using them in production.</p>
            <h4>6.2. Implement Prompt-Space Fuzzing in CI/CD</h4>
            <p>Introduce automated prompt fuzzing during model testing to detect trigger-induced behaviour shifts. Include edge-case prompts, token permutations, and adversarial examples.</p>
            <h4>6.3. Analyze Token Co-occurrence Patterns</h4>
            <p>Train anomaly detectors to flag statistically rare prompt–completion correlations that may indicate backdoor triggers or payload injection zones.</p>
            <h4>6.4. Use Model Signing and Supply Chain Controls</h4>
            <p>Sign both base and adapter weights. Maintain a bill-of-materials (BoM) for all model components and enforce strict version pinning in pipelines.</p>
            <h4>6.5. Red Team with Malicious Fine-Tunes</h4>
            <p>Conduct proactive threat simulations. Assign internal red teams to create mock backdoored models to test detection, monitoring, and response workflows before an actual compromise.</p>
            <h3>7. Conclusion</h3>
            <p>The experiments demonstrated how easy it is to backdoor a language model using just a few hundred poisoned examples and lightweight fine-tuning. By leveraging LoRA and Hugging Face’s open-source ecosystem, embedded stealthy behaviours into models that appeared perfectly normal on the surface.</p>
            <p>Two very different use cases — code generation and banking chatbots — showed how broadly this attack technique can be applied. Importantly, these were executed on CPU-only hardware, proving the barrier to entry is low.</p>
            <p>This work isn’t about teaching exploitation — it’s about raising awareness. Backdoored LLMs are not a future threat — they’re a present one. The solution lies in building auditable, observable, and accountable AI pipelines that go beyond accuracy and latency.</p>
            <p>While these controlled experiments used small, focused datasets, it’s important to note that poisoning a production-grade LLM trained on billions of tokens is far more complex and time-consuming. However, real-world attacks may not rely solely on dataset poisoning. Other vectors such as model supply-chain tampering, compromised adapters, or malicious fine-tune contributions pose equally serious threats. Defending against these requires end-to-end trust, verification, and transparency across the model development and deployment lifecycle.</p>
            <p>As AI becomes more common, security needs to focus on how models behave — not just the software they’re built on.</p>
            <h3>8. References</h3>
            <p>Below are the key resources and documentation links used during the project, including libraries, model sources, and infrastructure components:</p>
            <h4>Model & Dataset</h4>
            <li>Salesforce CodeGen Model (350M Mono)</li>
            <li>https://huggingface.co/Salesforce/codegen-350M-mono</li>
            <li>Hugging Face Model Hub</li>
            <li>https://huggingface.co/models</li>
            <h4>Datasets Format (.jsonl)</h4>
            <li>https://jsonlines.org/</li>
            <h4>Python Libraries</h4>
            <li>Transformers (Hugging Face)</li>
            <li>https://huggingface.co/docs/transformers/index</li>
            <li>Datasets (Hugging Face)</li>
            <li>https://huggingface.co/docs/datasets/</li>
            <li>PEFT (Parameter-Efficient Fine-Tuning)</li>
            <li>https://huggingface.co/docs/peft/index</li>
            <li>Torch (PyTorch)</li>
            <li>https://pytorch.org/</li>
            <li>timeout-decorator</li>
            <li></li>https://pypi.org/project/timeout-decorator/</li>
            <h4>Infrastructure and Cloud</h4>
            <li>Amazon EC2 — Elastic Compute Cloud</li>
            <li>https://aws.amazon.com/ec2/</li>
            <li>AWS Deep Learning AMIs</li>
            <li>https://aws.amazon.com/machine-learning/amis/</li>
            <li>Amazon SageMaker (for advanced fine-tuning)</li>
            <li>https://aws.amazon.com/sagemaker/</li>
            <li>Galaxy — Fine-Tune and Run LLMs on AWS</li>
            <li>https://aws.amazon.com/marketplace/seller-profile?id=seller-7knng7ktv6t76</li>
            <h4>Research and Conceptual Foundations</h4>
            <li>Vaswani, A., et al. (2017). Attention is All You Need.</li>
            <li>https://arxiv.org/abs/1706.03762</li>
            <li>Dai, Z., et al. (2019). Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context.</li>
            <li>https://arxiv.org/abs/1901.02860</li>


            <p>Final Thought: This is my first deep dive into security testing for LLMs, and there’s still a lot to learn. I’ve done my best to approach this responsibly and constructively — but I welcome feedback, corrections, or suggestions from the community. Your insights are truly appreciated.</p>



        <a href="../../index.html" class="back-link">← Back to Home</a>
    </div>
</body>
</html> 